---
title: "ðŸŒ¬ï¸ðŸ—³ Assignment 2: Wind Turbines, Matching, and Difference-in-Differences"
subtitle: "Replicate causal inference identification strategies in Stokes (2015) "
author: "Kylie Newcomer"
format:
  html:
    theme: sketchy
    css: styles.css
date: "February 4, 2026"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

### Assignment instructions

Working with classmates to troubleshoot code and concepts is encouraged. If you collaborate, list collaborators at the top of your submission.

All written responses must be written independently (in your own words).

Keep your work readable: Use clear headings and label plot elements thoughtfully.

Assignment submission: [Kylie Newcomer]{.underline}

------------------------------------------------------------------------

### Introduction

In this assignment you will be doing political weather forecasting except the â€œstormsâ€ we care about are electoral swings that might follow local wind turbine development.

In Stokes (2015), the idea is that a policy with diffuse benefits (cleaner electricity) can create concentrated local costs (turbines nearby), and those local opponents may â€œsend a signalâ€ at the ballot box (i.e., NIMBYISM). Your job is to use two statistical tools:

-   Matching: Can we create a more apples-to-apples comparison between precincts that did vs. did not end up near turbine proposals?
-   Fixed effects + Difference-in-Differences: Can we use repeated elections to estimate how within-precinct changes in turbine exposure relate to changes in incumbent vote share?

------------------------------------------------------------------------

### Learning goal: Replicate the matching and fixed effects analyses from study:

> Stokes (2015): *"Electoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy*.

-   **Study:** [Stokes (2015) - Article](https://drive.google.com/file/d/1y2Okzjq2EA43AW5JzCvFS8ecLpeP6NKh/view?usp=sharing)
-   **Data source:** [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

::: callout
`NOTE:` Replication of study estimates will be approximate. An alternative matching procedure and fixed effects estimation package are utilized in this assignment for illustration purposes.
:::

------------------------------------------------------------------------

### Setup: Load libraries

0.  Load libraries (+ install if needed)

```{r}

library(tidyverse)
library(here)
library(janitor)
library(jtools)

library(gtsummary)
library(gt)

library(MatchIt) # matching
library(cobalt)  # balance + love plots

library(fixest) # fast fixed effects
library(scales) # plotting

```

------------------------------------------------------------------------

### Part 1: Study Background

#### **1A.** Dive into the details of the study design and evaluation plan

> Goal: Get familiar with the study setting, environmental issue, and policy under evaluation.

::: callout
`NOTE:` Read over study to inform your response to the assignment questions. For this assignment we will skip-over sections that describe the *Instrumental Variables* identification strategy. We will cover instrumental variable designs weeks 6-7.
:::

**1A.Q1** Summarize the environmental policy issue, the outcome of interest, and the intervention being evaluated. Be sure to include a brief description of each of the following key elements of the study: unit of analysis, outcome, treatment, comparison group):

*Response:* This study examines the effects of proposed and operational wind turbines on the changes of votes for the incumbent political party in Ontario, Canada. The unit of analysis is the precincts, that are treated by proposed turbines and compared with precincts without proposed turbines. The outcome of the study is the percent of votes for the incumbent (liberal) party following three elections.

**1A.Q2** Why might turbine proposals be correlated with baseline political preferences or rural areas? Provide 2 plausible mechanisms, and explain why that creates confounding.

*Response:* The turbines would be funded by tax dollars, meaning increased taxes for residence. More left leaning communities are usually more supportive of increased taxes for community benefits and clean energy. Additionally, rural areas are better suited for wind turbines as they have more available land and more consistent wind from a lack of buildings.

------------------------------------------------------------------------

#### **1B.** Break down the causal inference strategy and identify threats to identification:

**1B.Q1** What is the key identifying assumption for a fixed effects / Difference-in-Difference design? Explain how this assumption when satisfied provides evidence of causal effect:

*Response:* The key assumption for fixed effects / DiD design is the parallel trends assumption.

**1B.Q2** What is the reason for using a fixed effects approach from a causal inference perspective? Summarize within the context of study (in your own words).

*Response:* The fixed effects approach attempts to control for other unobserved factors and time-varying confounders to further strengthen causal inference. In this study, the fixed effects account for differences in precincts and the election year.

**1B.Q3** What part of the SUTVA assumption is most likely violated in the context of this study design (and why)?

*Response:* The STUVA assumption is likely violated from spillover effect. It is likely that a proposed turbine in a different precinct would still impact other precincts.

**1B.Q4** Why does spillover matter when estimating an unbiased treatment effect?

*Response:* Spillover matters because it can result in the treatment effect skewing towards 0.

**1B.Q5** How do the authors assess the risk of spillovers, and what analytic choice do they make to attempt to mitigate the risk that spillover biases the causal estimate?

*Response:* In this study, spillover is regarded as spillover from precincts due to people seeing turbines from further away than the 3 km threshold.

------------------------------------------------------------------------

### Part 2: Matching

------------------------------------------------------------------------

We will start by evaluating the 2007 survey (cross-sectional) data. Treatment is defined by whether a precinct is near a turbine proposal (within 3 km).

> Goal: Match precincts using pre-treatment covariates and then estimate the effect of proposed wind turbines on incumbent vote share.

#### **2A.** Load data for matching

1.  Read in data file `stokes15_survey2007.csv`
2.  Code `precinct_id` and `district_id` as factors
3.  Take a look at the data

```{r}

match_data <- read_csv(here("data", "stokes15_survey2007.csv")) %>% 
    mutate(precinct_id = factor(precinct_id),
           district_id = factor(district_id))
    
```

**2A.Q1** Intuition check: **Why match?** Explain rationale for using this method.

*Response:* In this study there are far more untreated than treated precincts. By matching, we can validate causal inference and limit any bias through treatment assignment.

------------------------------------------------------------------------

#### **2B.** Check imbalance (before matching)

-   Create a covariate *balance table* comparing treated and control precincts
-   Treatment indicator: `proposed_turbine_3km`
-   Include pre-treatment covariates: `log_home_val_07`, `p_uni_degree`, `log_median_inc`, `log_pop_denc`
-   Use the `tbl_summary()` function from the `{gtsummary}` package.

```{r}

match_data %>% 
    select(proposed_turbine_3km, log_home_val_07, p_uni_degree, log_median_inc, log_pop_denc) %>% 
    tbl_summary(
    by = proposed_turbine_3km,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    )
  ) %>%
  modify_header(label ~ "**Covariate**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Group**")

```

**2B.Q1** Summarize the table output: Which covariates look balanced/imbalanced?

*Response:* The covariates home value, university degree, and median income seem balanced, while population density seems unbalanced.

**2B.Q2** Describe in your own words why these covariates might be expected to confound the treatment estimate:

*Response (2-4 sentences):* Often times, education level and economic class are usually indicators of political affiliation and are confounding with each other as higher education levels typically have a higher income. Additionally more densely popoulated cities tend to lean left politically, while more rural areas tend to be more right leaning.

------------------------------------------------------------------------

**2B.Q3** Intuition check: What type of data do you need to conduct a matching analysis?

*Response:* We need multiple pre-treatment covariates and many control observations.

------------------------------------------------------------------------

### Conduct matching estimation using the {`MatchIt`} package:

ðŸ“œ [Documentation - MatchIt](https://kosukeimai.github.io/MatchIt/)

Learning goals:

-   Approximate the Mahalanobis matching method used in Stokes (2015)
-   Implement another common matching approach called `propensity score matching`

::: callout
`NOTE`: In the replication code associated with Stokes (2015) the {`AER`} package is used for Mahalanobis matching. In this assignment we use the {`MatchIt`} package. The results are comparable but will not be exactly the same.
:::

------------------------------------------------------------------------

### 2C. Mahalanobis nearest-neighbor matching

-   Conduct Mahalanobis matching\
-   Use nearest-neighbor match without replacement using Mahalanobis distance
-   Use 1-to-1 matching (match one control unit to each treatment unit)
-   Extract the matched data using `match.data()`

```{r}
set.seed(2412026)

match_model <- matchit(
     proposed_turbine_3km ~ log_home_val_07 + 
         p_uni_degree + 
         log_median_inc + 
         log_pop_denc,
  data = match_data, 
  method = "nearest",       # Nearest neighbor matching
  distance = "mahalanobis", # Mahalanobis distance
  ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
  replace = FALSE           # Control observations are not replaced
)

# Extract matched data
matched_data <- match.data(match_model)

```

```{r}
summary(match_model)
```

**2C.Q1** Using the `summary()` output: Which covariate had the largest and smallest `Std. Mean Diff.` before matching. Next, compare largest/smallest `Std. Mean Diff.` after matching.

*Response:* Log median income had the smallest SMD (-0.0636) before matching, while log population density had the largest(-0.8897). The smallest largest values were consistent with the covariates, however both decreased dramatically at 0.0002 and -0.0329 respectively.

------------------------------------------------------------------------

#### 2D. Create a "love plot" using `love.plot()` â¤ï¸

ðŸ“œ [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

-   Plot mean differences for data before & after matching across all pre-treatment covariates
-   This is an effective way to evaluate how effective matching was at achieving balance.

------------------------------------------------------------------------

-   Make a love plot of standardized mean differences (SMDs) before vs after matching.
-   Include a threshold line at 0.1.
-   In love plot display `mean.diffs`

```{r}

new_names <- data.frame(
    old = c("log_home_val_07", "p_uni_degree", "log_median_inc", "log_pop_denc"),
    new = c("Home Value (log)", "Percent University Degree",
            "Median Income (log)", "Population Density (log)"))

# Love plot
love.plot(match_model, stats = "mean.diffs",
          thresholds = c(m = 0.1),
          var.names = new_names)

```

**2D.Q1** Interpret the love plot in your own words:

*Response:* After matching, the love plot shows that the difference in means of the covariates is adjusted towards 0. This means that the matching was balanced.

------------------------------------------------------------------------

### Propensity score matching

------------------------------------------------------------------------

#### 2E. Propensity Score Matching (PSM)

-   Estimate 1:1 nearest-neighbor Propensity Score Matching
-   Same code as above except change `distance = "logit"`

```{r}

set.seed(2412026)

propensity_scores <- matchit(
     proposed_turbine_3km ~ log_home_val_07 + 
         p_uni_degree + 
         log_median_inc + 
         log_pop_denc,
  data = match_data, 
  method = "nearest",       # Nearest neighbor matching
  distance = "logit",       # Logit distance
  ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
  replace = FALSE           # Control observations are not replaced
)
    
```

------------------------------------------------------------------------

#### Create table displaying covariate balance using `cobalt::bal.tab()`

ðŸ“œ [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

Use `bal.tab()` to report balance before and after matching.

```{r}
bal.tab(propensity_scores, 
        var.names = new_names) 

```

**2E.Q1** Compare Mahalanobis vs propensity score matching. Which method did a better job at achieving balance?

*Response:* The Mahalanobis matching was better because the difference in means were closer to 0 than the propensity score matching.

------------------------------------------------------------------------

#### 2F. Estimate an effect in the matched sample

Using the matched data (Mahalanobis method), estimate the effect of treatment on the change in incumbent vote share (`change_liberal`).

```{r}
reg_match <- lm(
    change_liberal ~ proposed_turbine_3km,
    data = matched_data,
    weights = weights
)

summ(reg_match, model.fit = FALSE)
```

**2F.Q1** Have you identified a causal estimate using this approach: Why or why not?

*Response:* We identified the causal estimate to be -0.06 with a significant p-value. Depsite a significant p-value, we cannot conclude if this estimate is accurate without taking into account unobserved variables.

**2F.Q2** When using a matching method, what is the main threat to causal identification?

*Response:* When using a matching model, we run the risk of not including unobserved confounders.

**2F.Q3** Describe why the treatment estimate represents the `Average Treatment for the Treated (ATT)` and explain why this is the case relative to estimation of the `Average Treatment Effect (ATE)`.

*Response:* In this scenario, the treatment estimates the effect for only the precincts that had a proposed turbine within 3km, thus representing the Average Treatment for Treated. This differs from the Average Treatment Effect which would look at the effects of the treatment (proposed wind turbines) across the entire population (all precincts).

------------------------------------------------------------------------

### Part 3: Panel Data, Fixed Effects, and Difference-in-Difference

**Data source:** [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

------------------------------------------------------------------------

#### **3A:** Read in the panel data + code variables `precinct_id` and `year` as factors

```{r}

panel_data <- read_csv(here("data", "Stokes15_panel_data.csv")) %>% 
    mutate(precinct_id = factor(precinct_id),
           year = factor(year))

# HINT: Try running `tabyl(panel_data$year)`. Review article to make sense of the row numbers (n).
tabyl(panel_data$year)
```

**3A.Q1:** Why are there 18,558 rows in `panel_data`?

*Response:* The panel data has a row for each of the 6,186 precincts across the 3 election years, resulting in 18,558 rows.
```{r}
# How many years are included in the panel?
n_distinct(panel_data$year)

# How many precincts are there?
n_distinct(panel_data$precinct_id)
```

**3A.Q2:** How many unique precincts are *ever treated* (i.e., `proposed` & `operational`)?

*Response:* 184 precincts are treated with proposed turbines and 52 precincts are treated with proposed turbines.

```{r}

panel_data %>%
  group_by(precinct_id) %>%
  summarise(
    ever_proposed    = any(proposed_turbine == 1, na.rm = TRUE),
    ever_operational = any(operational_turbine == 1, na.rm = TRUE),
    .groups = "drop") %>%
  summarise(
    n_ever_proposed    = sum(ever_proposed),
    n_ever_operational = sum(ever_operational))

```

------------------------------------------------------------------------

#### **3B.** Plot and evaluate parallel trends: Replicate `Figure.2` (Stokes, 2015)

1.  Create indicators for whether each precinct is ever treated by 2011 (`treat_p`, `treat_o`; separate indicator for proposals and operational turbines).
2.  Plot mean incumbent vote share by year for treated vs control precincts (with 95% CIs).
3.  Facet by turbine type (proposed & operational)

Step 1: Prepare data

```{r}

trends_data <- panel_data %>%
  group_by(precinct_id) %>%
  mutate(
    treat_p = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),  # ever proposed (in any year)
    treat_o = as.integer(any(operational_turbine == 1, na.rm = TRUE))) %>% # ever operational (in any year)
  ungroup() %>% 
  pivot_longer(c(treat_p, treat_o),
               names_to = "turbine_type", values_to = "treat") %>% 
  mutate(
      turbine_type = factor(turbine_type,
                            levels = c("treat_p", "treat_o"),
                            labels = c("Proposed turbines", "Operational turbines")),  
    status = if_else(treat == 1, "Treated", "Control"),
    year   = factor(year))

```

Step 2: Create trends plot

```{r}

pd <- position_dodge(width = 0.15)

trends_data %>%
  group_by(turbine_type, status, year) %>%
  summarise(
    mean = mean(perc_lib, na.rm = TRUE),
    n    = sum(!is.na(perc_lib)),
    se   = sd(perc_lib, na.rm = TRUE) / sqrt(n), 
    ci   = qt(.975, df = pmax(n - 1, 1)) * se,
    .groups = "drop") %>%
ggplot(aes(year, mean, color = status, group = status)) +
  geom_line(position = pd, linewidth = 1.2) +
  geom_point(position = pd, size = 2.6) +
  geom_errorbar(
    aes(ymin = mean - ci, ymax = mean + ci),
    position = pd, width = .12, linewidth = .7, color = "black") +
  facet_wrap(~ turbine_type, nrow = 1) +
  scale_color_manual(values = c(Control = "#0072B2", Treated = "#B22222")) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_cartesian(ylim = c(.20, .57)) +
  labs(
    title = "Figure 2. Trends in the Governing Partyâ€™s Vote Share",
    x = "Election Year",
    y = "Liberal Party Vote Share",
    color = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    strip.text = element_text(face = "bold"))

```

**3B.Q1:** Write a short paragraph assessing the parallel trends assumption for each outcome.

*Response (4-6 sentences):* The left figure displaying proposed turbines shows that 2003-2007, the trend of decreasing liberal party votes for both the treated and control variables which satisfies the parallel trends assumption. Following 2007 election, a sharper decline is seen in the treated group as opposed to the control group. For operational turbines, both the control and treated groups show a similar decreasing trend in liberal party votes again satisfying the parallel trends assumption. Despite having a higher percentage of liberal party votes in 2003, the treated group ends up with a larger decrease by the end of the 2011 election.

------------------------------------------------------------------------

### Estimating Fixed Effects Models (DiD) for proposals

$$
\text{Y}_{it}
=  \alpha_0 +
\beta \cdot (\text{proposed_turbine}_{it})
+ \gamma_i
+ \delta_t
+ \varepsilon_{it}
$$

-   $Y_{it}$ is the vote share for the Liberal Party in precinct *i* in time *t*
-   $\beta$ is the treatment effect of a turbine being proposed within a precinct
-   $\gamma_i$ is the precinct fixed effect
-   $\delta_t$ is the year fixed effect

------------------------------------------------------------------------

### Example 1: Randomly sample 40 precincts

-   To illustrate the "dummy variable method" of estimating fixed effects using the the general `lm()` function we are going to randomly sample 40 precincts (20 "treated" precincts with proposed turbines).
-   If we attempted to use this approach with the full sample estimating all 6185 (n-1) precinct-level coefficients is impractical (it would take a long time).

```{r}
set.seed(40002026)

precinct_frame <- panel_data %>%
  group_by(precinct_id) %>%
  summarise(
    proposed_turbine_any = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),
    .groups = "drop"
  )

ids_40 <- precinct_frame %>%
  group_by(proposed_turbine_any) %>%
  slice_sample(n = 20) %>%
  ungroup() %>%
  select(precinct_id)

sample_40_precincts <- panel_data %>%
  semi_join(ids_40, by = "precinct_id")

```

------------------------------------------------------------------------

#### **3C:** Estimate a fixed effects model using `lm()` with fixed effects added for `precinct` and `year` using the sample of 40 precincts just created.

```{r}
model1_ff <- lm(
   perc_lib ~ proposed_turbine + precinct_id + year,
    data = sample_40_precincts
)


```

```{r}
summ(model1_ff, model.fit = FALSE, digits = 3)
```

**3C.Q1:** Intuition check: Is the *signal-to-noise* ratio for the treatment estimate greater than *2-to-1*?

*Response:* The treatment estimate was -0.057 which is not greater than 2-to-1.

> HINT: Add the argument `digits = 3` to the `summ()` function above

```{r}
summ(model1_ff, model.fit = FALSE, robust = TRUE, digits = 3)
```

**3C.Q2:** Re-run the `summ()` function using the *heteroscedasticiy robust standard error adjustment* (`robust = TRUE`). Did the standard error (S.E.) estimates change? Explain why.

*Response:* The SE increased from 0.031 to **0.039**, most likely due to the fact that the variance of the residuals were not constant.

**3C.Q3:** Compare results of the model above to the findings from the fixed effects analysis in the Stokes (2015) study. Why might the results be similar or different?

*Response:* The Stokes 2015 study found that proposed wind turbines decreased the vote for the incumbent party by about 5%, which is similar to our results of a decrease by 5.7%. It is likely different due to the random sampling used to simulate the data.

**3C.Q4:** In your own words, explain why it is advantageous from a causal inference perspective to include year and precinct fixed effects. Explain how between-level and within-level variance is relevant to the problem of omitted variable bias (OVB).

*Response (2-4 sentences):* Including year a as fixed effect is better for a causal inference estimate because it accounts for the temporal variation between election years (4 year span). Accounting for precinct level mitigates individual precinct differences like population demographics and economic status. Both of these variables are likely to influence votes.

------------------------------------------------------------------------

#### **3D.** Now using the full sample, estimate the treatment effect of wind turbine proposals on incumbent vote share. Use `feols()` from the `{fixest}` package to estimate the fixed effects.

See vignette here: [fixest walkthrough](https://cran.r-project.org/web/packages/fixest/vignettes/fixest_walkthrough.html#11_Estimation)

```{r}

model2_ff <- feols(
    perc_lib ~ proposed_turbine,
    data = panel_data
)

summary(model2_ff, cluster = ~precinct_id)
```

**3D.Q1:** Interpret the model results and translate findings to be clear to an audience that may not have a background in causal inference (Econometrics) methods.

In panel data settings, why is clustering by precinct important (i.e., `cluster = ~precinct_id`) ?â€

*Response (4-6 sentences):* On average, proposed turbines within 3km of a precinct are estimated to decrease the incumbent vote share by 10% in that precinct. This is taking into account the fixed effects of unobserved variables that may impact indirectly impact the voting results. Clustering the precincts is important because the precinct are repeated in the data for each year. This means that observations of precinct each year are not independent of each other and must be accounted for.

------------------------------------------------------------------------

#### **3E.** Estimate the treatment effect of *operational wind turbines* on incumbent vote share. Use the same approach as the previous model.

```{r}

model3_ff <- feols(perc_lib ~ operational_turbine, 
                  data = panel_data)

summary(model3_ff, cluster = ~precinct_id)
```

**3E.Q1:** Interpret the `model3_ff` results as clearly and **concisely** as you can.

*Response:* On average, operational turbines decrease the vote for the incumbent party by 7.87%.

**3E.Q2:** Why do you think the effect of proposed wind turbines is different from operational wind turbines. Develop your own theory about why incumbent vote share is affected in this way. Use the Stokes (2015) study to inform your response as needed.

*Response:* It is likely that operational wind turbines had a larger negative effect on incumbent vote share than proposed wind turbines because the NIMBY theory suggests that proximity to infrastructure can lead to opposing further developments. This supports operational turbines having a larger impact of votes than proposed turbines, because community members are seeing these turbines in their "backyard", whereas a proposed turbine is theoretical and not yet physically perceived.

------------------------------------------------------------------------

```{r, message=TRUE, echo=FALSE, eval=FALSE}

library(praise); library(cowsay)

praise("${EXCLAMATION}! ðŸš€ Great work - You are ${adjective}! ðŸ’«")

say("The End", "duck")
```
